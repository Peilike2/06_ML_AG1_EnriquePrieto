{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6 - Selección_de_características.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ti5Wnz3cgkRD"},"source":["# 6 - Selección de Características mediante Clustering"]},{"cell_type":"markdown","metadata":{"id":"3wbjJ1PrhsIJ"},"source":["Como problema de partida de ejemplo de como aplicar diferentes algoritmos de clustering vamos a utilizar unos datos del Producto Interior Bruto de los Estados de Estados Unidos en 2009. Los datos están en [Enlace](https://drive.google.com/open?id=1Rb_7eNacmktJM3RqETQmSLysXYjk5EDG)"]},{"cell_type":"code","metadata":{"id":"QcqAjsMIKluO"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import itertools\n","import seaborn as sns\n","from numpy import corrcoef, transpose, arange\n","from pylab import pcolor, show, colorbar, xticks, yticks\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fvxDPxPhheSW"},"source":["## Carga de Datos"]},{"cell_type":"markdown","metadata":{"id":"AygZwiVvg6WE"},"source":["En primer lugar cargamos los datos dentro del entorno con las funcionalidades que nos permite Google Colab"]},{"cell_type":"code","metadata":{"id":"RBK-O9mTgglR"},"source":["# 0.1 load data from file\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jvD1yDjhASV"},"source":["Posteriormente utilizamos la librería [Pandas](https://pandas.pydata.org/) con el fin de poder manejar los datos en una estructura denominada DataFrame\n"]},{"cell_type":"code","metadata":{"id":"4QQ6WZ3qhJLw"},"source":["# 0.2 load data in DataFrame\n","import pandas as pd\n","import io\n","df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')), index_col='State')\n","df_feat = transpose(df)\n","df_feat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2IVR7NZrh6Qe"},"source":["Estos datos contienen para cada uno de los estados (*State*) las diferentes cantidades en producto interior bruto dentro de categorías que denominaríamos de nivel 2 de agregación, es decir, son una desagragción de niveles superiores. De esta forma encontramos datos como el PIB de Agricultura, Minería, Construcción, Comercio, etc. "]},{"cell_type":"markdown","metadata":{"id":"dLHVkSQYiaCE"},"source":["Una vez que contamos con el DataFrame de Pandas podríamos analizar qué características necesitamos y cuales deberíamos excluir."]},{"cell_type":"code","metadata":{"id":"6gt2vGOooxow"},"source":["names =  df_feat.index\n","names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0hDALmpjjaP"},"source":["# Correlación entre variables\n","\n","Con el fin de que obtener un mejor rendimiento en el algoritmo de clustering será necesario identificar aquellas variables que son redundantes, es decir, se puede asumir que representan lo mismo, en este caso se puede utilizar el análisis de correlaciones. "]},{"cell_type":"code","metadata":{"id":"sQMg8ny9kKw6"},"source":["#https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\n","R = corrcoef(df_feat)\n","\n","# http://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html\n","# Generate a mask for the upper triangle\n","sns.set(style=\"white\")\n","mask = np.zeros_like(R, dtype=np.bool)\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Set up the matplotlib figure\n","f, ax = plt.subplots(figsize=(11, 9))\n","\n","# Generate a custom diverging colormap\n","cmap = sns.diverging_palette(200, 10, as_cmap=True)\n","\n","# Draw the heatmap with the mask and correct aspect ratio\n","sns.heatmap(R, mask=mask, cmap=cmap, vmax=.8,\n","            square=True, xticklabels=names, yticklabels=names,\n","            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CB8NMV2Iw1Vu"},"source":["Las variables están correlacionadas entre ellas, pero no demasiado (no llega a 1) esto puede hacer que las sigamos teniendo en cuenta aunque hay que ser consciente que algunas de ellas pueden ser muy parecidas."]},{"cell_type":"markdown","metadata":{"id":"WbLxTTqEmeVT"},"source":["# Análisis de componentes principales\n","\n","Con el fin de observar como están relacionadas las variables también podemos hacer una representación en PCA de las mismas\n"]},{"cell_type":"code","metadata":{"id":"fgIvukA5mmZ4"},"source":["#1. Normalization of the data\n","#http://scikit-learn.org/stable/modules/preprocessing.html\n","from sklearn import preprocessing \n","min_max_scaler = preprocessing.MinMaxScaler()\n","features_norm = min_max_scaler.fit_transform(df_feat)\n","\n","\n","\n","#1.2. Principal Component Analysis\n","from sklearn.decomposition import PCA\n","estimator = PCA (n_components = 2)\n","X_pca = estimator.fit_transform(features_norm)\n","print(\"Variance Ratio: \", estimator.explained_variance_ratio_) \n","\n","\n","import matplotlib.pyplot as plt\n","fig, ax = plt.subplots()\n","for i in range(len(X_pca)):\n","    plt.text(X_pca[i][0], X_pca[i][1], names[i]) \n","\n","\n","plt.xlim(-2, 6)\n","plt.ylim(-1, 1.5)\n","ax.grid(True)\n","fig.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZySXBK2JnuoZ"},"source":["El ratio de variabilidad por eje es bueno, con el eje X se representa casi el 80% de la variabilidad del conjunto de datos. \n","\n","Una vez que hemos verificado que el resultado es fiable podemos ver que muchas de las variables son muy parecidas entre ellas, y son algunas de ellas como \n","*Mining*, *Manufacturing*, *Finance*, *State_1* y *Buss. Services* los que son distintos de las demás y podrían ser las seleccionadas para realizar un clustering posterior.\n","\n","En este caso no nos interesa observar qué combinación lineal da forma a cada componente ya que no nos interesa seleccionar aquellos estados que son más relevantes.\n"]},{"cell_type":"markdown","metadata":{"id":"-rt9e6cEoELq"},"source":["# Clustering jerárquico\n","\n","Dado que tenemos pocos elementos que estudiar podemos utilizar clustering jerárquico para observar las relaciones de similitud entre el desarrollo de las variables. Aquellas grupos de variables que sean similares entre ellas pueden ser resumidas escogiendo una de ellas y de esa forma reduciríamos la dimensionalidad del conjunto de datos.\n","\n","Para evitar la maldición de la dimensionalidad se puede hacer el clustering con los resultados del análisis de componentes principales, aunque hay que ser precavido ya que aunque reducida, existe perdida de variabilidad en los datos cuando se hace la proyección."]},{"cell_type":"code","metadata":{"id":"5EPB_5Jrp56F"},"source":["# 2. Compute the similarity matrix\n","#http://docs.scipy.org/doc/scipy/reference/cluster.html\n","from scipy import cluster\n","import sklearn.neighbors\n","dist = sklearn.neighbors.DistanceMetric.get_metric('euclidean')\n","matdist= dist.pairwise(features_norm)\n","\n","\n","# 3.1.1 Visualization\n","import seaborn as sns; sns.set()\n","ax = sns.heatmap(matdist,vmin=0, vmax=1, yticklabels = names, xticklabels = names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h98gFECtqG9Z"},"source":["Al graficar la matriz de distancia podemos ver que las variables entre si no son demasiado parecidas, es más parece que hay hay cierta variablidad entre ellas salvo ciertos grupos (\"parejas\") de variables"]},{"cell_type":"code","metadata":{"id":"u51szGJ0odfA"},"source":["# 3. Building the Dendrogram\t\n","# http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n","clusters = cluster.hierarchy.linkage(matdist, method = 'single')\n","# http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html\n","cluster.hierarchy.dendrogram(clusters, color_threshold = 3, labels = names , leaf_rotation=90)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVjBDq5NsP8L"},"source":["Vamos a obtener la asignación de grupos de cada uno de ellos"]},{"cell_type":"code","metadata":{"id":"3F3xfkjisWYC"},"source":["cut = 3 # !!!! ad-hoc\n","labels = cluster.hierarchy.fcluster(clusters, cut , criterion = 'distance')\n","\n","labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"azL5Sj8QpG9I"},"source":["Hemos utilizado Single Link con el propósito de aislar rápidamente aquellos elementos que son outliers y que van a ser las características más significativas, en este caso *Finance*, *Manufacturing* y *Mining*. Luego se distinguen dos grupos, uno de propósito general y otro con *Education*, *Buss. Services*, *State.1*"]},{"cell_type":"code","metadata":{"id":"hIzJ-itOsK6D"},"source":["#plotting orginal points with color related to label\n","plt.scatter(X_pca[:,0], X_pca[:,1], c=labels,s=50)\n","for i in range(len(X_pca)):\n","    plt.text(X_pca[i][0], X_pca[i][1], names[i][0:3]) \n","\n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4T7aBVB5JcXw"},"source":["# DBSCAN\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZN6iN_6Yqbeq"},"source":["Vamos a identificar elementos outliers mediante la utilización de DBSCAN. El propósito de identificar ouliter es aislar aquellas características que son diferentes a los demás y que nos pueden ayudar a distinguir a los diferentes elementos a los que representan."]},{"cell_type":"markdown","metadata":{"id":"_hmMkF14qNBK"},"source":["### Parametrización\n"]},{"cell_type":"markdown","metadata":{"id":"krU2oRXhKE2s"},"source":["Vamos a fijar MinPts = 3 y observar que epsilon podemos establecer\n"]},{"cell_type":"code","metadata":{"id":"-I7kPBEWKNP-"},"source":["minPts=2\n","from sklearn.neighbors import kneighbors_graph\n","A = kneighbors_graph(features_norm, minPts, include_self=False)\n","Ar = A.toarray()\n","\n","seq = []\n","for i,s in enumerate(features_norm):\n","    for j in range(len(features_norm)):\n","        if Ar[i][j] != 0:\n","            seq.append(matdist[i][j])\n","            \n","seq.sort()\n","# establecer intervalo ejes\n","fig = plt.figure()\n","ax = fig.gca()\n","ax.set_xticks(np.arange(0, 120, 10))\n","ax.set_yticks(np.arange(0, 3, 0.2))\n","\n","plt.plot(seq)\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"syyC0088K5iL"},"source":["Vamos a establecer un rango amplio de pruebas teniendo en cuenta que buscamos aislar elementos diferentes a los demás. "]},{"cell_type":"markdown","metadata":{"id":"CE2ZWAU9YaPt"},"source":["## Ejecución"]},{"cell_type":"code","metadata":{"id":"OqGMe4lDK3g7"},"source":["from sklearn.cluster import DBSCAN\n","\n","for eps in np.arange(0.50, 1.8, 0.20):\n","  db = DBSCAN(eps, min_samples=minPts).fit(features_norm)\n","  core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n","  core_samples_mask[db.core_sample_indices_] = True\n","  labels = db.labels_\n","  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","  n_outliers = list(labels).count(-1)\n","  print (\"%6.2f, %d, %d\" % (eps, n_clusters_, n_outliers))\n","  \n","#labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XYrltM-0u9y3"},"source":["Como podemos observar DBSCAN nos agrupa los elementos en un grupo y el resto son outliers. El número de outliers si es significativo tenerlo en cuenta, por lo que nos vamos a quedar con las parametrizaciones que nos ofrecen un mayor número de outliers."]},{"cell_type":"code","metadata":{"id":"ce989cxzIJHl"},"source":["\n","db = DBSCAN(eps=0.9, min_samples=minPts).fit(features_norm)\n","labels = db.labels_\n","labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19D9aCoCqtks"},"source":["Visualizamos los resultados"]},{"cell_type":"code","metadata":{"id":"YEbIQqaYQL_e"},"source":["#plotting orginal points with color related to label\n","plt.scatter(X_pca[:,0], X_pca[:,1], c=labels,s=50)\n","for i in range(len(X_pca)):\n","    plt.text(X_pca[i][0], X_pca[i][1], names[i][0:3]) \n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UjyFrBnCqvv_"},"source":["Y aunque podríamos proceder a su intepretación en este caso lo que vamos a hacer es identificar aquellos elementos que son outliers"]},{"cell_type":"code","metadata":{"id":"3QELcpJfmOWT"},"source":["df_feat['dbscan_group'] = labels\n","\n","df_feat[df_feat['dbscan_group'] == -1]"],"execution_count":null,"outputs":[]}]}